# Configuration file for the heart disease classification pipeline

# Random seed for reproducibility
random_state: 42

# Data configuration
data:
    # Relative path to the raw CSV dataset (heart.csv).  If you
    # downloaded the Kaggle dataset, copy heart.csv into this
    # directory or update this path accordingly.
    raw_file: data/raw/Heart_disease_cleveland_new.csv

    # Directories for intermediate and processed data
    interim_dir: data/interim
    processed_dir: data/processed

    # Train/validation/test split sizes
    test_size: 0.1
    val_size: 0.1  # fraction of the remainder after test split

# Feature engineering configuration
feature_engineering:
    # Whether to compute new ratio features
    add_ratio_features: true

# Model configuration
model:
    # List of models to train.  Each entry is keyed by a model name
    # recognised by `HeartDiseaseClassifier`.  You can extend this list
    # to include additional algorithms such as decision trees, SVMs,
    # AdaBoost, XGBoost, LightGBM, KMeans and gradient boosting.
    algorithms:
        - random_forest
        - logistic_regression
        - knn
        - decision_tree
        - svm_rbf
        - adaboost
        - xgboost
        - lightgbm
        - kmeans
        - gradient_boosting
    # Hyperparameters for each model.  See heart_classifier.py for defaults.
    params:
        random_forest:
            n_estimators: [100, 200]
            max_depth: [null, 5, 10]
            min_samples_split: [2, 5]
            min_samples_leaf: [1, 2]
        logistic_regression:
            C: [0.1, 1.0, 10.0]
            penalty: [l1, l2]
            class_weight: [null, balanced]
        knn:
            n_neighbors: [3, 5, 7]
            weights: [uniform, distance]
            p: [1, 2]
        decision_tree:
            max_depth: [null, 5, 10]
            min_samples_split: [2, 5, 10]
            min_samples_leaf: [1, 2, 4]
            class_weight: [null, balanced]
        svm_rbf:
            C: [0.1, 1.0, 10.0]
            gamma: [0.01, 0.1, 1.0]
            class_weight: [null, balanced]
        adaboost:
            n_estimators: [50, 100, 200]
            learning_rate: [0.01, 0.1, 1.0]
        xgboost:
            # Reduced hyperparameter grid for faster training; adjust as needed
            n_estimators: [200]
            max_depth: [3]
            learning_rate: [0.1]
            subsample: [0.8]
            colsample_bytree: [0.8]
        lightgbm:
            # Reduced hyperparameter grid for faster training; adjust as needed
            n_estimators: [200]
            learning_rate: [0.1]
            max_depth: [5]
            num_leaves: [31]
            subsample: [0.8]
            colsample_bytree: [0.8]
        kmeans:
            n_clusters: [2, 3, 4]
        gradient_boosting:
            n_estimators: [200, 500]
            learning_rate: [0.01, 0.1, 0.2]
            max_depth: [3, 5]
